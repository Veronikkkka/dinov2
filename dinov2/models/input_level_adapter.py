# -*- coding: utf-8 -*-
"""Input_level_adapter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/167em6SRMZhY0L7Lo-zY2ibkV0qQ7kMWJ
"""

import torch.nn as nn
import os
import torch
from torch import Tensor
from typing import Optional, Tuple, List
from torch.nn.functional import grid_sample, conv2d, interpolate, pad as torch_pad

class Kernel_Predictor(nn.Module):
    def __init__(self, dim, mode='low', num_heads=1, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = qk_scale or head_dim ** -0.5
        # Query Adaptive Learning (QAL)
        self.q = nn.Parameter(torch.rand((1, 4, dim)), requires_grad=True)

        self.kv_downsample = nn.Sequential(
            nn.Conv2d(4, dim // 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),
            nn.BatchNorm2d(dim // 8),
            nn.GELU(),
            nn.Conv2d(dim // 8, dim // 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),
            nn.BatchNorm2d(dim // 4),
            nn.GELU(),
            nn.Conv2d(dim // 4, dim // 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),
            nn.BatchNorm2d(dim // 2),
            nn.GELU(),
            nn.Conv2d(dim // 2, dim, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),
            nn.BatchNorm2d(dim),
        )
        self.k = nn.Linear(dim, dim, bias=qkv_bias)
        self.v = nn.Linear(dim, dim, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        self.down = nn.Linear(dim, 1)
        self.softmax = nn.Softmax(dim=2)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

        # Basic Parameters Number
        if mode == 'low':
            self.gain_base = nn.Parameter(torch.FloatTensor([3]), requires_grad=True)
        else:
            self.gain_base = nn.Parameter(torch.FloatTensor([1]), requires_grad=True)


        self.r1_base = nn.Parameter(torch.FloatTensor([3]), requires_grad=False)
        self.r2_base = nn.Parameter(torch.FloatTensor([2]), requires_grad=False)

    def forward(self, x):
        d_x = self.kv_downsample(x).flatten(2).transpose(1, 2)
        B, N, C = d_x.shape
        k = self.k(d_x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        v = self.v(d_x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)

        q = self.q.expand(B, -1, -1).view(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        out = (attn @ v).transpose(1, 2).reshape(B, 4, C)
        out = self.proj(out)
        out = self.proj_drop(out)
        out = self.down(out).squeeze(-1)

        out = torch.unbind(out, 1)
        r1, r2, gain, sigma = out[0], out[1], out[2], out[3]
        r1 = 0.1 * r1 +  self.r1_base
        r2 = 0.1 * r2 +  self.r2_base

        gain =gain + self.gain_base

        return r1, r2, gain, self.sigmoid(sigma)

class Matrix_Predictor(nn.Module):
    def __init__(self, dim, num_heads=1, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = qk_scale or head_dim ** -0.5
        # Query Adaptive Learning (QAL)
        self.q = nn.Parameter(torch.rand((1, 16 + 1, dim)), requires_grad=True)
        self.kv_downsample = nn.Sequential(
            nn.Conv2d(4, dim // 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),
            nn.BatchNorm2d(dim // 8),
            nn.GELU(),
            nn.Conv2d(dim // 8, dim // 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),
            nn.BatchNorm2d(dim // 4),
            nn.GELU(),
            nn.Conv2d(dim // 4, dim // 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),
            nn.BatchNorm2d(dim // 2),
            nn.GELU(),
            nn.Conv2d(dim // 2, dim, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),
            nn.BatchNorm2d(dim),
        )
        self.k = nn.Linear(dim, dim, bias=qkv_bias)
        self.v = nn.Linear(dim, dim, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        self.down = nn.Linear(dim, 1)
        self.softmax = nn.Softmax(dim=2)
        self.relu = nn.ReLU()
        self.ccm_base = nn.Parameter(torch.eye(4), requires_grad=False)

    def forward(self, x):
        d_x = self.kv_downsample(x).flatten(2).transpose(1, 2)
        B, N, C = d_x.shape
        k = self.k(d_x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        v = self.v(d_x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)

        q = self.q.expand(B, -1, -1).view(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        out = (attn @ v).transpose(1, 2).reshape(B, 16 + 1, C)
        out = self.proj(out)
        out = self.proj_drop(out)
        out = self.down(out)
        out, distance = out[:, :16, :], out[:, 16:, :].squeeze(-1)
        out = out.view(B, 4, 4)
        # print(self.ccm_base)
        # print(out)

        ccm_matrix = 0.1 * out + self.ccm_base
        distance = self.relu(distance) + 1

        return ccm_matrix, distance

class NILUT(nn.Module):
    """
    Simple residual coordinate-based neural network for fitting 3D LUTs
    Official code: https://github.com/mv-lab/nilut
    """
    def __init__(self, in_features=3, hidden_features=32, hidden_layers=3, out_features=3, res=True):
        super().__init__()

        self.res = res
        self.net = []
        self.net.append(nn.Linear(in_features, hidden_features))
        self.net.append(nn.ReLU())

        for _ in range(hidden_layers):
            self.net.append(nn.Linear(hidden_features, hidden_features))
            self.net.append(nn.Tanh())

        self.net.append(nn.Linear(hidden_features, out_features))
        if not self.res:
            self.net.append(torch.nn.Sigmoid())

        self.net = nn.Sequential(*self.net)

    def forward(self, intensity):
        output = self.net(intensity)
        if self.res:
            output = output + intensity
            output = torch.clamp(output, 0.,1.)
        return output

def _assert_image_tensor(img: Tensor) -> None:
    if not img.ndim >= 2:
        raise TypeError("Tensor is not a torch image.")

def _get_gaussian_kernel1d(kernel_size: int, sigma: float) -> Tensor:
    ksize_half = (kernel_size - 1) * 0.5

    x = torch.linspace(-ksize_half, ksize_half, steps=kernel_size).to(sigma.device)
    #print(x.device)
    #print(sigma.device)
    pdf = torch.exp(-0.5 * (x / sigma).pow(2))
    kernel1d = pdf / pdf.sum()

    return kernel1d


def _get_gaussian_kernel2d(
    kernel_size: List[int], sigma: List[float], dtype: torch.dtype, device: torch.device
) -> Tensor:
    kernel1d_x = _get_gaussian_kernel1d(kernel_size[0], sigma[0]).to(device, dtype=dtype)
    kernel1d_y = _get_gaussian_kernel1d(kernel_size[1], sigma[1]).to(device, dtype=dtype)
    kernel2d = torch.mm(kernel1d_y[:, None], kernel1d_x[None, :])
    return kernel2d

def _cast_squeeze_in(img: Tensor, req_dtypes: List[torch.dtype]) -> Tuple[Tensor, bool, bool, torch.dtype]:
    need_squeeze = False
    # make image NCHW
    if img.ndim < 4:
        img = img.unsqueeze(dim=0)
        need_squeeze = True

    out_dtype = img.dtype
    need_cast = False
    if out_dtype not in req_dtypes:
        need_cast = True
        req_dtype = req_dtypes[0]
        img = img.to(req_dtype)
    return img, need_cast, need_squeeze, out_dtype

def _cast_squeeze_out(img: Tensor, need_cast: bool, need_squeeze: bool, out_dtype: torch.dtype) -> Tensor:
    if need_squeeze:
        img = img.squeeze(dim=0)

    if need_cast:
        if out_dtype in (torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64):
            # it is better to round before cast
            img = torch.round(img)
        img = img.to(out_dtype)

    return img

def gaussian_blur(img: Tensor, kernel_size: List[int], sigma: List[float]) -> Tensor:
    if not (isinstance(img, torch.Tensor)):
        raise TypeError(f"img should be Tensor. Got {type(img)}")

    _assert_image_tensor(img)

    dtype = img.dtype if torch.is_floating_point(img) else torch.float32
    kernel = _get_gaussian_kernel2d(kernel_size, sigma, dtype=dtype, device=img.device)
    kernel = kernel.expand(img.shape[-3], 1, kernel.shape[0], kernel.shape[1])

    img, need_cast, need_squeeze, out_dtype = _cast_squeeze_in(
        img,
        [
            kernel.dtype,
        ],
    )

    # padding = (left, right, top, bottom)
    padding = [kernel_size[0] // 2, kernel_size[0] // 2, kernel_size[1] // 2, kernel_size[1] // 2]
    img = torch_pad(img, padding, mode="reflect")
    img = conv2d(img, kernel, groups=img.shape[-3])

    img = _cast_squeeze_out(img, need_cast, need_squeeze, out_dtype)
    return img

def anisotropic_gaussian_blur(
    img: Tensor, 
    kernel_size: List[int], 
    r1: Tensor, 
    r2: Tensor, 
    theta: float = 0.0
) -> Tensor:
    """
    Apply anisotropic Gaussian blur to each channel of the input image
    """
    if not isinstance(img, torch.Tensor):
        raise TypeError(f"img should be Tensor. Got {type(img)}")

    _assert_image_tensor(img)
    
    B, C, H, W = img.shape
    dtype = img.dtype if torch.is_floating_point(img) else torch.float32
    
    per_channel_kernels = (r1.ndim > 1 and r1.shape[1] == C)

    padding = [kernel_size[0] // 2, kernel_size[0] // 2, kernel_size[1] // 2, kernel_size[1] // 2]
    img_padded = torch_pad(img, padding, mode="reflect")    
    img_padded = img_padded.to(dtype)
    
    # process each channel independently
    if per_channel_kernels:
        output = []
        for c in range(C):
            channel_r1 = r1[:, c]
            channel_r2 = r2[:, c]
            
            kernel = _get_anisotropic_gaussian_kernel2d(
                kernel_size, channel_r1, channel_r2, theta, dtype, img.device
            )
            
            # (B, 1, kernel_h, kernel_w)
            kernel = kernel.unsqueeze(1)
            
            # (B, 1, H+padding, W+padding)
            channel = img_padded[:, c:c+1]
            
            channel_output = []
            for b in range(B):

                b_kernel = kernel[b:b+1]
                b_channel = channel[b:b+1]                
                b_output = conv2d(b_channel, b_kernel, groups=1)
                channel_output.append(b_output)
            
            channel_output = torch.cat(channel_output, dim=0)
            output.append(channel_output)
        
        # combine channels
        output = torch.cat(output, dim=1)
    else:
        # same kernel for all channels
        kernel = _get_anisotropic_gaussian_kernel2d(
            kernel_size, r1, r2, theta, dtype, img.device
        )
        
        output = []
        for b in range(B):
            # (C, 1, kernel_h, kernel_w)
            b_kernel = kernel[b:b+1].expand(C, 1, kernel_size[0], kernel_size[1])
            
            # grouped conv
            b_output = conv2d(img_padded[b:b+1], b_kernel, groups=C)
            output.append(b_output)
        
        output = torch.cat(output, dim=0)
    
    if img.dtype != dtype:
        output = output.to(img.dtype)
    
    return output

def _get_anisotropic_gaussian_kernel2d(
    kernel_size: List[int], r1: Tensor, r2: Tensor, theta: float = 0.0, 
    dtype: torch.dtype = torch.float32, device: torch.device = None
) -> Tensor:
    """
    Creates an anisotropic 2D Gaussian kernel
    """
    if device is None:
        device = r1.device
    
    batch_size = r1.shape[0]
    kernel_height, kernel_width = kernel_size
    
    # coordinate grid
    x_grid = torch.linspace(-(kernel_width - 1) / 2, (kernel_width - 1) / 2, kernel_width, device=device)
    y_grid = torch.linspace(-(kernel_height - 1) / 2, (kernel_height - 1) / 2, kernel_height, device=device)
    y, x = torch.meshgrid(y_grid, x_grid, indexing='ij')
    
    # for batch processing
    x = x.expand(batch_size, -1, -1)
    y = y.expand(batch_size, -1, -1)
    
    # equation (3) in the paper
    cos_theta = torch.cos(torch.tensor(theta, device=device))
    sin_theta = torch.sin(torch.tensor(theta, device=device))
    
    # Calculate b0, b1, b2 coefficients with broadcasting for batch support
    r1_squared = r1.view(batch_size, 1, 1).pow(2)
    r2_squared = r2.view(batch_size, 1, 1).pow(2)
    
    b0 = (cos_theta.pow(2) / (2 * r1_squared)) + (sin_theta.pow(2) / (2 * r2_squared))
    b1 = (torch.sin(2 * torch.tensor(theta, dtype=torch.float32)) / 4) * ((r1 / r2).pow(2) - 1) / r1_squared
    b2 = (sin_theta.pow(2) / (2 * r1_squared)) + (cos_theta.pow(2) / (2 * r2_squared))
    
    # Compute kernel values using equation (2) from the paper
    kernel = torch.exp(-(b0 * x.pow(2) + 2 * b1 * x * y + b2 * y.pow(2)))
    
    # Normalize the kernel
    kernel = kernel / kernel.sum(dim=(1, 2), keepdim=True)
    
    return kernel.to(dtype=dtype)

# def Gain_Denoise(I1, r1, r2, gain, sigma, k_size=3):  # [9, 9] in LOD dataset, [3, 3] in other dataset
#     out = []
#     for i in range(I1.shape[0]):
#         I1_gain = gain[i] * I1[i,:,:,:]
#         # blur = gaussian_blur(I1_gain, \
#         #                         [k_size, k_size], \
#         #                         [r1[i], r2[i]])
#         blur = anisotropic_gaussian_blur(I1_gain, \
#                                 [k_size, k_size], \
#                                 [r1[i], r2[i]])
#         sharp = blur + sigma[i] * (I1[i,:,:,:] - blur)
#         out.append(sharp)
#     return torch.stack([out[i] for i in range(I1.shape[0])], dim=0)

def Gain_Denoise(I1, r1, r2, gain, sigma, k_size=3):
    """
    Apply gain adjustment and anisotropic Gaussian denoising to RGGB raw images.
    
    Args:
        I1: Input RGGB raw image of shape (B, 4, H, W)
        r1: Major axis radius of shape (B,) or (B, 4) for channel-specific parameters
        r2: Minor axis radius of shape (B,) or (B, 4) for channel-specific parameters
        gain: Gain factor of shape (B,) or (B, 4) for channel-specific gains
        sigma: Filter parameter for detail preservation of shape (B,)
        k_size: Kernel size for Gaussian blur
    
    Returns:
        Processed image of same shape as input
    """
    B, C, H, W = I1.shape
    
    # Check if we have channel-specific parameters
    channel_specific = False
    if isinstance(gain, torch.Tensor) and gain.ndim > 1:
        channel_specific = True
    
    # Process each batch sample
    out = []
    for i in range(B):
        # Apply gain adjustment
        if channel_specific:
            # Apply channel-specific gains
            gain_factor = gain[i].view(C, 1, 1)  # Shape: [4, 1, 1]
            I1_gain = I1[i] * gain_factor
        else:
            # Apply same gain to all channels
            I1_gain = gain[i] * I1[i]
        
        # Apply anisotropic Gaussian blur for denoising
        if channel_specific:
            # Get channel-specific r1 and r2 parameters
            sample_r1 = r1[i].unsqueeze(0)  # Shape: [1, 4]
            sample_r2 = r2[i].unsqueeze(0)  # Shape: [1, 4]
        else:
            # Use the same r1 and r2 for all channels
            sample_r1 = r1[i].unsqueeze(0)  # Shape: [1]
            sample_r2 = r2[i].unsqueeze(0)  # Shape: [1]
        
        # Add batch dimension back for the blur function
        I1_gain_with_batch = I1_gain.unsqueeze(0)  # Shape: [1, 4, H, W]
        
        # Apply anisotropic blur
        blur = anisotropic_gaussian_blur(
            I1_gain_with_batch,
            [k_size, k_size],
            sample_r1,
            sample_r2
        ).squeeze(0)  # Remove batch dim, result shape: [4, H, W]
        
        # Detail preservation with sigma parameter
        sigma_i = sigma[i]
        sharp = blur + sigma_i * (I1[i] - blur)
        
        out.append(sharp)
    
    return torch.stack(out, dim=0)

# Shades of Gray and Colour Constancy (Graham D. Finlayson, Elisabetta Trezzi)
# def SoG_algo(img, p=1):
#     # https://library.imaging.org/admin/apis/public/api/ist/website/downloadArticle/cic/12/1/art00008
#     img = img.permute(1,2,0)       # (C,H,W) --> (H,W,C)

#     img_P = torch.pow(img, p)

#     R_avg = torch.mean(img_P[:,:,0]) ** (1/p)
#     G_avg = torch.mean(img_P[:,:,1]) ** (1/p)
#     B_avg = torch.mean(img_P[:,:,2]) ** (1/p)

#     Avg = torch.mean(img_P) ** (1/p)

#     R_avg = R_avg / Avg
#     G_avg = G_avg / Avg
#     B_avg = B_avg / Avg

#     img_out = torch.stack([img[:,:,0]/R_avg, img[:,:,1]/G_avg, img[:,:,2]/B_avg], dim=-1)

#     return img_out

def SoG_algo(img, p=1):
    img = img.permute(1,2,0)  # (C,H,W) --> (H,W,C)
    
    img_P = torch.pow(img, p)
    
    # Now calculate the average for each channel
    avg_channels = torch.mean(img_P, dim=(0, 1)) ** (1/p)  # Average for each channel

    Avg = torch.mean(img_P) ** (1/p)

    # Normalize each channel
    img_out = torch.stack([img[:,:,i]/(avg_channels[i] / Avg) for i in range(img.shape[2])], dim=-1)

    return img_out

# def WB_CCM(I2, ccm_matrix, distance):
#     out_I3 = []
#     out_I4 = []
#     for i in range(I2.shape[0]):
#         # SOG White Balance Algorithm
#         I3 = SoG_algo(I2[i,:,:,:], distance[i])

#         # Camera Color Matrix
#         I4 = torch.tensordot(I3, ccm_matrix[i,:,:], dims=[[-1], [-1]])
#         I4 = torch.clamp(I4, 1e-5, 1.0)

#         out_I3.append(I3)
#         out_I4.append(I4)

#     return  torch.stack([out_I3[i] for i in range(I2.shape[0])], dim=0), \
#             torch.stack([out_I4[i] for i in range(I2.shape[0])], dim=0)

class WB_CCM_Adapter(nn.Module):
    def __init__(self):
        super(WB_CCM_Adapter, self).__init__()
        # Linear transformation from 4 channels to 3 channels
        self.channel_reduction = nn.Conv2d(4, 3, kernel_size=1)

    def forward(self, I2, ccm_matrix, distance):
        out_I3 = []
        out_I4 = []
        for i in range(I2.shape[0]):
            # SOG White Balance Algorithm
            I3 = SoG_algo(I2[i,:,:,:], distance[i])

            # Camera Color Matrix
            I4 = torch.tensordot(I3, ccm_matrix[i,:,:], dims=[[-1], [-1]])
            I4 = torch.clamp(I4, 1e-5, 1.0)

            # Change the input shape to [B, C, H, W] format
            # I4 = I4.permute(0, 3, 1, 2)  # From [B, H, W, C] to [B, C, H, W]

            # Apply the learned channel reduction
            # I4 = self.channel_reduction(I4)

            out_I3.append(I3)
            out_I4.append(I4)

        return  torch.stack([out_I3[i] for i in range(I2.shape[0])], dim=0), \
                torch.stack([out_I4[i] for i in range(I2.shape[0])], dim=0)



class Input_level_Adapeter(nn.Module):
    def __init__(self, mode='normal', lut_dim=32, out='all', k_size=3, w_lut=True):
        super(Input_level_Adapeter, self).__init__()
        '''
        mode: normal (for normal & over-exposure conditions) or low (for low-light conditions)
        lut_dim: implicit neural look-up table dim number
        out: if all, return I1, I2, I3, I4, I5, if not all, only return I5
        k_size: denosing kernel size, must be odd number, we set it to 9 in LOD dataset and 3 in other dataset
        w_lut: with or without implicit 3D Look-up Table
        '''

        self.Predictor_K = Kernel_Predictor(dim=64, mode=mode)
        self.Predictor_M = Matrix_Predictor(dim=64)
        self.w_lut = w_lut
        if self.w_lut:
            self.LUT = NILUT(hidden_features=lut_dim)
        self.out = out
        self.k_size = k_size
        self.WB_CCM = WB_CCM_Adapter()
        self.channel_reduction = nn.Conv2d(4, 3, kernel_size=1)



    def forward(self, I1):
        # (1). I1 --> I2: Denoise & Enhancement & Sharpen
        r1, r2, gain, sigma = self.Predictor_K(I1)
        I2 = Gain_Denoise(I1, r1, r2, gain, sigma, k_size=self.k_size)  # (B,C,H,W)
        I2 = torch.clamp(I2, 1e-5, 1.0) # normal & over-exposure

        ccm_matrix, distance = self.Predictor_M(I2)

        # (2). I2 --> I3: White Balance, Shade of Gray
        # (3). I3 --> I4: Camera Colour Matrix Transformation
        I3, I4 = self.WB_CCM(I2, ccm_matrix, distance) # (B,H,W,C)
        

        if self.w_lut:
        # (4). I4 --> I5: Implicit Neural LUT
            I4_reduced = self.channel_reduction(I4.permute(0,3,1,2))  # Convert to [B,C,H,W] and reduce to 3 channels
            I5 = self.LUT(I4_reduced.permute(0,2,3,1))  # Convert back to [B,H,W,C] for LUT
            I5 = I5.permute(0,3,1,2) 
            # I5 = self.LUT(I4).permute(0,3,1,2)

            # # I5 = I5.permute(2, 0, 1).unsqueeze(0)  # [H, W, C] → [1, C, H, W]
            # I5 = self.channel_reduction(I5)        # [1, 3, H, W]
            # # I5 = I5.squeeze(0).permute(1, 2, 0)    # [1, 3, H, W] → [H, W, 3]

            if self.out == 'all':   # return all features
                return [I1, I2, I3.permute(0,3,1,2), I4.permute(0,3,1,2), I5]
            else:   # only return I5
                return [I5]

        else:
            if self.out == 'all':
                return [I1, I2, I3.permute(0,3,1,2), I4.permute(0,3,1,2)]
            else:
                return [I4.permute(0,3,1,2)]




if __name__ == "__main__":
    os.environ['CUDA_VISIBLE_DEVICES']='4'

    input = torch.rand([4,3,512,512])
    net = Input_level_Adapeter(out='all', w_lut=False)
    out = net(input)

